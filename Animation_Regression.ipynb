{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53184c6",
   "metadata": {},
   "source": [
    "# Animation Regression\n",
    "- This is the notebook dedicated to making the model for all animation movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27239b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the many imports needed to make the dataset, and then make the model for the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import dill\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from random import uniform\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D\n",
    "from keras import regularizers\n",
    "from random import sample\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02e8d6",
   "metadata": {},
   "source": [
    "## Loading the Raw Data\n",
    "this includes the master dataframe of all the movies and the master dictionaries for all of the actors and directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acfefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_genres_df.pkl', 'rb') as f: \n",
    "    films = pickle.load(f)\n",
    "    \n",
    "with open('wd_popularity.pkl', 'rb') as f:\n",
    "    wd_pop = pickle.load(f)\n",
    "wd_pop_names = list(wd_pop.keys())\n",
    "    \n",
    "with open('actor_popularity.pkl', 'rb') as f:\n",
    "    actor_pop = pickle.load(f)\n",
    "cast_pop_names = list(actor_pop.keys())\n",
    "\n",
    "all_dict = wd_pop | actor_pop\n",
    "for key, val in list(all_dict.items()):\n",
    "    all_dict[key]['average'] = round(np.mean(list(val.values())))\n",
    "    for key1, val1 in list(val.items()):\n",
    "        if val1 >= 1000000:\n",
    "            all_dict[key][key1] = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c0c7d",
   "metadata": {},
   "source": [
    "## Setting up the Transformer Objects\n",
    "It was necessary for the objects to be instantiated seperately so that I didn't accidentaly clear them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d113bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_genres = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb_languages = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb_country = MultiLabelBinarizer(sparse_output=True)\n",
    "ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "vect_act = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase = True)\n",
    "vect_dir = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase = True)\n",
    "vect_write = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase = True)\n",
    "vect_key = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase = True)\n",
    "vect_prod = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase = True)\n",
    "\n",
    "ACT_DATA = []\n",
    "DIR_DATA = []\n",
    "WRITE_DATA = []\n",
    "KEY_DATA = []\n",
    "PROD_DATA = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a7bed",
   "metadata": {},
   "source": [
    "## Making the Dataset\n",
    "This is where I make a dataset specifically for this movie genre, as well as formatting for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e561291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will return a dataset of the specified genre, where the people columns are of the minimum popularity\n",
    "def make_dataset(genre, min_pop):\n",
    "    tdf = deepcopy(films)\n",
    "    \n",
    "    # here I expand the genres column first so that I can reduce the dataframe to just the columns of the specified genre\n",
    "    tdf = tdf.join(pd.DataFrame.sparse.from_spmatrix(mlb_genres.fit_transform(tdf.pop('genres')), index = tdf.index, columns = mlb_genres.classes_))\n",
    "    tdf = tdf[tdf[genre] == 1]\n",
    "    \n",
    "    # here I get rid of rows and columns that are unnecessary \n",
    "    tdf = tdf[tdf['num_review'] > 10]\n",
    "    tdf = tdf.reset_index().drop('index', axis =1)\n",
    "    tdf = tdf.drop(['Biography', 'Documentary', 'Film-Noir', 'Music', 'Musical', 'News', 'Reality-TV', 'Sport'], axis = 1)\n",
    "    \n",
    "    # these are the popularity columns that will be filled in later\n",
    "    tdf['dir_pop'] = 0\n",
    "    tdf['write_pop'] = 0\n",
    "    tdf['cast_pop'] = 0      \n",
    "    \n",
    "    # expanding the languages and country columns\n",
    "    tdf = tdf.join(pd.DataFrame.sparse.from_spmatrix(mlb_languages.fit_transform(tdf.pop('languages')), index = tdf.index, columns = mlb_languages.classes_), lsuffix = '_left')\n",
    "    tdf = tdf.join(pd.DataFrame.sparse.from_spmatrix(mlb_country.fit_transform(tdf.pop('country')), index = tdf.index, columns = mlb_country.classes_), lsuffix = '_left')\n",
    "\n",
    "    # expanding and reducing the prod_comp, key_words, actors, director, and writer columns\n",
    "    tdf = dim_reduction(tdf, 'prod_comp', 1, min_pop)\n",
    "    tdf = dim_reduction(tdf, 'key_words', 10, min_pop)\n",
    "    # this variable is a placeholder so I know which columns represent individual people\n",
    "    temp = tdf.shape[1]\n",
    "    tdf = dim_reduction(tdf, 'actors', 2, min_pop)\n",
    "    tdf = dim_reduction(tdf, 'director', 1, min_pop)\n",
    "    tdf = dim_reduction(tdf, 'writer', 1, min_pop)  \n",
    "\n",
    "    # here I add in the movie rating columns for \"R\", \"PG-13\", etc..\n",
    "    rating = pd.DataFrame(ohe.fit_transform(tdf[['m_rating']]).toarray(), columns = ohe.get_feature_names(['m_rating']))\n",
    "    rating.columns = [col[9:] for col in list(rating.columns)]\n",
    "    rating.index = tdf.index\n",
    "    tdf = pd.concat([tdf, rating], 1).drop('m_rating', axis =1)\n",
    "    \n",
    "    # list of all of the poeple involved in the movie\n",
    "    people_list = list(tdf.columns)[temp-4:]\n",
    "    \n",
    "    return(tdf, people_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fafb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is for reducing the number of columns from the dataframe so that the outliers can be removed\n",
    "# in this case, outliers would be actors/directors/writers that did not participate in many movies\n",
    "\n",
    "def dim_reduction(baseline_df, column, min_freq, min_pop):\n",
    "    # each column needs a seperate mlb object so that we can use the seperate obejects later to correctly predict new movies\n",
    "    # that are not in the dataset\n",
    "    if column == 'actors':\n",
    "        vectorizer = vect_act\n",
    "    elif column == 'director':\n",
    "        vectorizer = vect_dir\n",
    "    elif column == 'writer':\n",
    "        vectorizer = vect_write\n",
    "    elif column == 'key_words':\n",
    "        vectorizer = vect_key\n",
    "    elif column == 'prod_comp':\n",
    "        vectorizer = vect_prod\n",
    "        \n",
    "    # this is where I set up the dataframe that will show which columns are important and which are not\n",
    "    count_data = vectorizer.fit_transform(baseline_df[column])    \n",
    "    movie_data = pd.DataFrame(data = count_data.toarray(), index = baseline_df.index, columns = vectorizer.get_feature_names())\n",
    "    # this will add the role of the person to their string name so that you will know if that person acted as a director vs writer vs actor\n",
    "    movie_data.columns = [name + ' ' + column for name in list(movie_data.columns)]\n",
    "        \n",
    "    # big list will act as the list of how freqent one name shows up in the column\n",
    "    big_list = movie_data.sum()\n",
    "    # good data will be the columns that will be kept in the ultimate genre dataframe\n",
    "    good_data = []\n",
    "\n",
    "    # here I get rid of all names that showed up less than or equal to the min_freq input or are more popular than the min_pop\n",
    "    for data in big_list.index:\n",
    "        data_freq = big_list[data]\n",
    "        try:\n",
    "            # this retreives the average popularity for a person across their whole career\n",
    "            data_pop = all_dict[data.replace((' ' + column), '')]['average']\n",
    "        except:\n",
    "            # if I don't have any info on the person then the actor popularity number will not help\n",
    "            data_pop = 1000000\n",
    "            \n",
    "        # the popularity metric means that the lower the score, the more popular you are, I will flip this later\n",
    "        if (data_freq > min_freq) | (data_pop < min_pop):\n",
    "            good_data.append(data)\n",
    "    \n",
    "    # I need to save which columns are being used for later when i make predictions on individual movies that are note in the dataframe\n",
    "    if column == 'actors':\n",
    "        for col in good_data:\n",
    "            ACT_DATA.append(col)\n",
    "    elif column == 'director':\n",
    "        for col in good_data:\n",
    "            DIR_DATA.append(col)\n",
    "    elif column == 'writer':\n",
    "        for col in good_data:\n",
    "            WRITE_DATA.append(col)\n",
    "    elif column == 'key_words':\n",
    "        for col in good_data:\n",
    "            KEY_DATA.append(col)\n",
    "    elif column == 'prod_comp':\n",
    "        for col in good_data:\n",
    "            PROD_DATA.append(col)\n",
    "            \n",
    "    # returning the whole dataset minus the columns that were irrelavent\n",
    "    return baseline_df.join(movie_data[good_data], lsuffix = '_left').drop(column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e027365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df, p_list = make_dataset('Animation', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1fa275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2503, 9213)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae414cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6345"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9020ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the unfinished dataframe for the sake of caution, in case of a crash\n",
    "with open('./Animation/animation_raw_df.pkl', 'wb') as f:\n",
    "    pickle.dump(raw_df, f)\n",
    "\n",
    "with open('./Animation/animation_people.pkl', 'wb') as f:\n",
    "    pickle.dump(p_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79fbd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2503, 9213)\n",
      "5074\n"
     ]
    }
   ],
   "source": [
    "# loading option so you don't have to run everything above\n",
    "with open('./Animation/animation_raw_df.pkl', 'rb') as f:\n",
    "    raw_df = pickle.load(f)\n",
    "print(raw_df.shape)\n",
    "\n",
    "with open('./Animation/animation_dict.pkl', 'rb') as f:\n",
    "    p_list = pickle.load(f)\n",
    "print(len(p_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5a35",
   "metadata": {},
   "source": [
    "## Adding the Popularity Metric\n",
    "Here we use the popularity scores to get the total popularity of the actors, writers, and directors involved in each film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50cdbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will get a subset of the dataframe that contains movies of the specified person, and then updates popularity column\n",
    "def popularity(name, df):\n",
    "    # getting the dictionary for the individual person\n",
    "    pop_scores = all_dict[name]\n",
    "    \n",
    "    # preparing several lists to find the right scores in dictionary of scores\n",
    "    act_df = df[df[name] == 1]\n",
    "    years = act_df.release_date.tolist()\n",
    "    cast_pop = act_df.cast_pop.tolist()\n",
    "    dir_pop = act_df.dir_pop.tolist()\n",
    "    write_pop = act_df.write_pop.tolist()\n",
    "    \n",
    "    update_pop = []\n",
    "    for row in act_df.release_date:\n",
    "        try:\n",
    "            update_pop.append(1000000 - pop_scores[row.year])\n",
    "        except:\n",
    "            update_pop.append(1000000 - pop_scores['average'])\n",
    "\n",
    "    # Check the name to choose which column we update\n",
    "    if ' actors' in name:\n",
    "        act_df.cast_pop = list(np.array(cast_pop) + np.array(update_pop))\n",
    "    elif ' director' in name: \n",
    "        act_df.dir_pop = list(np.array(dir_pop) + np.array(update_pop))\n",
    "    elif ' writer' in name: \n",
    "        act_df.write_pop = list(np.array(write_pop) + np.array(update_pop))\n",
    "      \n",
    "    return pd.concat([df[df[name] != 1], act_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5579fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_dataset(tdf, p_list):\n",
    "    count = 0\n",
    "    for person in p_list:\n",
    "        count += 1\n",
    "        tdf = popularity(person, tdf)\n",
    "        print(count, end = '\\r')\n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# be warned this cell can take up to 16 hours to run depending on the CPU\n",
    "final = finish_dataset(raw_df, p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6d2382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cast_pop</th>\n",
       "      <th>dir_pop</th>\n",
       "      <th>write_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>WALL·E</td>\n",
       "      <td>6964588.0</td>\n",
       "      <td>998400.0</td>\n",
       "      <td>1988070.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Harry Potter and the Deathly Hallows: Part 2</td>\n",
       "      <td>9995935.0</td>\n",
       "      <td>998981.0</td>\n",
       "      <td>1992823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>9925218.0</td>\n",
       "      <td>999272.0</td>\n",
       "      <td>1993839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>Spirited Away</td>\n",
       "      <td>2988255.0</td>\n",
       "      <td>999226.0</td>\n",
       "      <td>999226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>8952885.0</td>\n",
       "      <td>999476.0</td>\n",
       "      <td>1996762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>After School Midnighters</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>Patchwork Family</td>\n",
       "      <td>2915346.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Stompa, selvfølgelig!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>Hoofmeisie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>The Seven Ravens</td>\n",
       "      <td>917946.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>795480.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4449 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title   cast_pop   dir_pop  \\\n",
       "1830                                        WALL·E  6964588.0  998400.0   \n",
       "3325  Harry Potter and the Deathly Hallows: Part 2  9995935.0  998981.0   \n",
       "3673         Harry Potter and the Sorcerer's Stone  9925218.0  999272.0   \n",
       "3328                                 Spirited Away  2988255.0  999226.0   \n",
       "3324       Harry Potter and the Chamber of Secrets  8952885.0  999476.0   \n",
       "...                                            ...        ...       ...   \n",
       "1757                      After School Midnighters        0.0       0.0   \n",
       "1640                              Patchwork Family  2915346.0       0.0   \n",
       "1748                         Stompa, selvfølgelig!        0.0       0.0   \n",
       "4234                                    Hoofmeisie        0.0       0.0   \n",
       "4090                              The Seven Ravens   917946.0       0.0   \n",
       "\n",
       "      write_pop  \n",
       "1830  1988070.0  \n",
       "3325  1992823.0  \n",
       "3673  1993839.0  \n",
       "3328   999226.0  \n",
       "3324  1996762.0  \n",
       "...         ...  \n",
       "1757        0.0  \n",
       "1640        0.0  \n",
       "1748        0.0  \n",
       "4234        0.0  \n",
       "4090   795480.0  \n",
       "\n",
       "[4449 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see that everything worked as expected\n",
    "final.sort_values('num_review', ascending = False)[['title', 'cast_pop', 'dir_pop', 'write_pop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407cd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Animation/Animation_df.pkl', 'rb') as f:\n",
    "    final = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37d3476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>num_review</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>TV-13</th>\n",
       "      <th>TV-14</th>\n",
       "      <th>TV-G</th>\n",
       "      <th>TV-MA</th>\n",
       "      <th>TV-PG</th>\n",
       "      <th>TV-Y</th>\n",
       "      <th>TV-Y7</th>\n",
       "      <th>TV-Y7-FV</th>\n",
       "      <th>UNK</th>\n",
       "      <th>Unrated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>15038118</td>\n",
       "      <td>Blue's Big City Adventure</td>\n",
       "      <td>2022-11-18 00:00:00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>424.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>21909950</td>\n",
       "      <td>Hawa</td>\n",
       "      <td>2022-12-01 00:00:00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>15565714</td>\n",
       "      <td>David and the Elves</td>\n",
       "      <td>2021-12-06 00:00:00</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>798.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>17081112</td>\n",
       "      <td>Xiong chu mo: Chong fan di qiu</td>\n",
       "      <td>2022-02-01 00:00:00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>17660740</td>\n",
       "      <td>The Pez Outlaw</td>\n",
       "      <td>2022-03-12 00:00:00</td>\n",
       "      <td>85.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>238.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>0232465</td>\n",
       "      <td>The Watermill Princess 2</td>\n",
       "      <td>2000-02-03 00:00:00</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>1706591</td>\n",
       "      <td>The Devil's Bride</td>\n",
       "      <td>2011-03-27 00:00:00</td>\n",
       "      <td>90.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1100119</td>\n",
       "      <td>The Notebook II</td>\n",
       "      <td>2009-08-21 00:00:00</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>2150209</td>\n",
       "      <td>My Grandfather's People</td>\n",
       "      <td>2011-11-25 00:00:00</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>0476735</td>\n",
       "      <td>My Father and My Son</td>\n",
       "      <td>2005-11-18 00:00:00</td>\n",
       "      <td>112.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>87000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4449 rows × 15682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                           title         release_date  runtime  \\\n",
       "73    15038118       Blue's Big City Adventure  2022-11-18 00:00:00     82.0   \n",
       "236   21909950                            Hawa  2022-12-01 00:00:00    104.0   \n",
       "312   15565714             David and the Elves  2021-12-06 00:00:00    105.0   \n",
       "440   17081112  Xiong chu mo: Chong fan di qiu  2022-02-01 00:00:00     97.0   \n",
       "508   17660740                  The Pez Outlaw  2022-03-12 00:00:00     85.0   \n",
       "...        ...                             ...                  ...      ...   \n",
       "1759   0232465        The Watermill Princess 2  2000-02-03 00:00:00    110.0   \n",
       "4085   1706591               The Devil's Bride  2011-03-27 00:00:00     90.0   \n",
       "319    1100119                 The Notebook II  2009-08-21 00:00:00     89.0   \n",
       "725    2150209         My Grandfather's People  2011-11-25 00:00:00    126.0   \n",
       "2536   0476735            My Father and My Son  2005-11-18 00:00:00    112.0   \n",
       "\n",
       "      imdb_rating  num_review  Action  Adventure  Animation  Comedy  ...  \\\n",
       "73            6.4       424.0       0          0          0       1  ...   \n",
       "236           5.1       220.0       0          0          0       1  ...   \n",
       "312           5.2       798.0       0          0          0       1  ...   \n",
       "440           5.9       165.0       0          0          1       1  ...   \n",
       "508           7.5       238.0       0          0          0       1  ...   \n",
       "...           ...         ...     ...        ...        ...     ...  ...   \n",
       "1759          4.0       336.0       0          0          0       1  ...   \n",
       "4085          4.6       124.0       0          0          0       0  ...   \n",
       "319           4.7      7600.0       0          1          0       1  ...   \n",
       "725           8.0     11000.0       0          0          0       1  ...   \n",
       "2536          8.2     87000.0       0          0          0       0  ...   \n",
       "\n",
       "      TV-13  TV-14  TV-G  TV-MA  TV-PG  TV-Y  TV-Y7  TV-Y7-FV  UNK  Unrated  \n",
       "73      0.0    0.0   0.0    0.0    0.0   1.0    0.0       0.0  0.0      0.0  \n",
       "236     0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "312     0.0    0.0   1.0    0.0    0.0   0.0    0.0       0.0  0.0      0.0  \n",
       "440     0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "508     0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "...     ...    ...   ...    ...    ...   ...    ...       ...  ...      ...  \n",
       "1759    0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "4085    0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "319     0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  0.0      0.0  \n",
       "725     0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  1.0      0.0  \n",
       "2536    0.0    0.0   0.0    0.0    0.0   0.0    0.0       0.0  0.0      0.0  \n",
       "\n",
       "[4449 rows x 15682 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see the dataframe is as expected\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22227b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f12bbf",
   "metadata": {},
   "source": [
    "## Starting the Model\n",
    "here is the start of the modeling process using Neural Networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.drop(['id', 'title'], axis = 1)\n",
    "df.release_date = pd.to_datetime(df.release_date)\n",
    "df.release_date = df.release_date.apply(lambda x: x.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2aca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic train test split for the modeling\n",
    "X = df.drop('imdb_rating', axis = 1)\n",
    "y = df['imdb_rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1764)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97532f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "ss = StandardScaler()\n",
    "x_train_ss_scaled = pd.DataFrame(ss.fit_transform(X_train), columns=X_train.columns)\n",
    "x_test_ss_scaled = pd.DataFrame(ss.transform(X_test),columns = X_test.columns)\n",
    "x_val_ss_scaled = pd.DataFrame(ss.transform(X_val),columns = X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cd2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this PCA is very important for reducing the amount of time it takes for the model to run\n",
    "pca = PCA(n_components = 7000, random_state = 42)\n",
    "pca_x_train = pca.fit_transform(x_train_ss_scaled)\n",
    "pca_x_val = pca.transform(x_val_ss_scaled)\n",
    "pca_x_test = pca.transform(x_test_ss_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1792f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we check to see that at least 95% of the variance is captured by the pca\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting in a decaying learning rate lets the model run faster and more accurate\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa1da7",
   "metadata": {},
   "source": [
    "### The Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310fe52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add in early stopping to make sure the model does not run longer than it has too\n",
    "early = [EarlyStopping(monitor='loss', min_delta = 0.001, patience = 20, mode = 'min'),\n",
    "        ModelCheckpoint(filepath = './Romance/romance_model.h5', monitor = 'loss', save_best_only = True),\n",
    "        LearningRateScheduler(scheduler)]\n",
    "opt = Adam(learning_rate = 0.0015)\n",
    "\n",
    "pca_ss_model = Sequential()\n",
    "pca_ss_model.add(Conv1D(filters = 20, kernel_size = (3), activation = 'relu', input_shape = (7000,1), \n",
    "                      padding = 'causal', kernel_initializer = 'normal'))\n",
    "pca_ss_model.add(MaxPooling1D(pool_size = (2)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "\n",
    "pca_ss_model.add(Conv1D(filters = 10, kernel_size = (3), activation = 'tanh', padding = 'causal'))\n",
    "pca_ss_model.add(MaxPooling1D(pool_size = (2)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "\n",
    "pca_ss_model.add(Flatten())\n",
    "\n",
    "pca_ss_model.add(Dense(128, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.001)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(100, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(64, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(45, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(32, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(16, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(8, activation = 'relu', kernel_initializer = 'normal', kernel_regularizer = regularizers.l2(0.005)))\n",
    "pca_ss_model.add(Dropout(0.3))\n",
    "pca_ss_model.add(Dense(1, kernel_initializer = 'normal'))\n",
    "\n",
    "pca_ss_model.compile(loss = 'mse', optimizer = opt, metrics = ['mae'])\n",
    "\n",
    "pca_ss_result = pca_ss_model.fit(pca_x_train, \n",
    "                                y_train.values, \n",
    "                                validation_data = [pca_x_val, y_val.values],\n",
    "                                epochs = 100, \n",
    "                                callbacks = early, \n",
    "                                batch_size = 70)\n",
    "\n",
    "y_test_pred = pca_ss_model.predict(pca_x_test)\n",
    "\n",
    "this_mse = mean_squared_error(y_test, y_test_pred)\n",
    "this_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print(this_mse, this_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ceda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will allow us to visulaize the results of the Neural Network\n",
    "def visualize_training_results(results):\n",
    "    history = results.history\n",
    "    plt.figure()\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history['val_mae'])\n",
    "    plt.plot(history['mae'])\n",
    "    plt.legend(['val_mae', 'mae'])\n",
    "    plt.title('MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55543ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training_results(pca_ss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the results for the best model that the Neural Network produced\n",
    "best_pca_ss_model = load_model('./Romance/romance_model.h5')\n",
    "best_y_test_pred = best_pca_ss_model.predict(pca_x_test)\n",
    "\n",
    "print(mean_squared_error(best_y_test_pred, y_test))\n",
    "print(mean_squared_error(best_y_test_pred, y_test, squared = False))\n",
    "print(mean_absolute_error(best_y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0fc2b",
   "metadata": {},
   "source": [
    "## Saving the Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we need to save all of the objects used to transform the dataset so that we can do it on any future movies\n",
    "with open('./Romance/mlb_genres.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb_genres, f)\n",
    "with open('./Romance/mlb_languages.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb_languages, f)\n",
    "with open('./Romance/mlb_country.pkl', 'wb') as f:\n",
    "    pickle.dump(mlb_country, f)\n",
    "with open('./Romance/ohe.pkl', 'wb') as f:\n",
    "    pickle.dump(ohe, f)\n",
    "with open('./Romance/vect_act.pkl', 'wb') as f:\n",
    "    dill.dump(vect_act, f)\n",
    "with open('./Romance/vect_dir.pkl', 'wb') as f:\n",
    "    dill.dump(vect_dir, f)\n",
    "with open('./Romance/vect_write.pkl', 'wb') as f:\n",
    "    dill.dump(vect_write, f)\n",
    "with open('./Romance/vect_key.pkl', 'wb') as f:\n",
    "    dill.dump(vect_key, f)\n",
    "with open('./Romance/vect_prod.pkl', 'wb') as f:\n",
    "    dill.dump(vect_prod, f)\n",
    "# with open('./Romance/ss.pkl', 'wb') as f:\n",
    "#     pickle.dump(ss, f)\n",
    "# with open('./Romance/pca.pkl', 'wb') as f:\n",
    "#     pickle.dump(pca, f)\n",
    "    \n",
    "with open('./Romance/actor_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(ACT_DATA, f)\n",
    "with open('./Romance/director_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(DIR_DATA, f)\n",
    "with open('./Romance/writer_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(WRITE_DATA, f)\n",
    "with open('./Romance/key_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(KEY_DATA, f)\n",
    "with open('./Romance/prod_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(PROD_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41763c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
