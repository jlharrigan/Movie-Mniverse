{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02def138",
   "metadata": {},
   "source": [
    "# IMDB Scraper\n",
    "this notebook is purely for scraping the movie info off of imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b225d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from random import uniform\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5b7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will contain the urls for all of the movies in the dataset\n",
    "all_u = []\n",
    "\n",
    "# this keeps track of movies already scraped so that ther are no duplicates\n",
    "completed = []\n",
    "completed_keys = []\n",
    "\n",
    "# this is where the info for each movie will be stored as a list of dicts until it can be transformed into a dataframe\n",
    "basic_rows = []\n",
    "key_word_rows = []\n",
    "\n",
    "loc_rows = []\n",
    "\n",
    "next_page = ''\n",
    "user_agents = [ \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "    'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) Gecko/20100101 Firefox/106.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:106.0) Gecko/20100101 Firefox/106.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; rv:106.0) Gecko/20100101 Firefox/106.0',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:107.0) Gecko/20100101 Firefox/107.0',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:107.0) Gecko/20100101 Firefox/107.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:107.0) Gecko/20100101 Firefox/107.0',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:107.0) Gecko/20100101 Firefox/107.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; rv:107.0) Gecko/20100101 Firefox/107.0',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.56',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.42',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c19362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets all of the urls for all the moves of a specified genre\n",
    "def get_all_genre_urls(genre, max_movies, dataset):\n",
    "    genre_url = 'https://www.imdb.com/search/title/?title_type=movie&genres=' + genre + '&explore=title_type,genres&ref_=adv_prv'\n",
    "    req_cat = requests.get(genre_url)\n",
    "    soup_cat = BeautifulSoup(req_cat.content, 'html.parser')\n",
    "    movies = soup_cat.find_all('div', class_='lister-item-image float-left')\n",
    "    for movie in movies:\n",
    "        if movie.a['href'] not in dataset:\n",
    "            dataset.append(movie.a['href'])\n",
    "\n",
    "    next_page = soup_cat.find('a', class_ = 'lister-page-next next-page')['href']\n",
    "    \n",
    "    for num in np.arange(max_movies):\n",
    "        print(num)\n",
    "        next_page = get_urls(next_page, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d56323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all the urls from one page of movies\n",
    "def get_urls(page_url, data):\n",
    "    req = requests.get('https://www.imdb.com' + page_url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    movies = soup.find_all('div', class_='lister-item-image float-left')\n",
    "    for movie in movies:\n",
    "        if movie.a['href'] not in data:\n",
    "            data.append(movie.a['href'])\n",
    "    return soup.find('a', class_ = 'lister-page-next next-page')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0e47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all of the info for one movie page\n",
    "def get_movie(movie_url):\n",
    "    if movie_url in completed:\n",
    "        return 0\n",
    "\n",
    "    time.sleep(uniform(0,2))\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent, 'Accept-Language': 'en-US,en;q=0.5'} \n",
    "    req = requests.get('https://www.imdb.com/' + movie_url, headers = headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    \n",
    "    if str(req) != '<Response [200]>':\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        title = soup.find('h1', class_ = 'sc-b73cd867-0 eKrKux').text\n",
    "    except:\n",
    "        try: \n",
    "            title = soup.find('h1', class_ = 'sc-b73cd867-0 fbOhB').text\n",
    "        except:\n",
    "            try:\n",
    "                title = soup.find('h1', class_ = 'sc-b73cd867-0 cAMrQp').text\n",
    "            except:\n",
    "                title = 'none'\n",
    "    try:\n",
    "        wr = soup.find_all('ul', role = \"presentation\", class_ = 'ipc-inline-list ipc-inline-list--show-dividers ipc-inline-list--inline ipc-metadata-list-item__list-content baseAlt')\n",
    "        try:\n",
    "            writers = [w.text.split('(')[0] + ' ' + w.a['href'].split('nm')[1].split('/?')[0] for w in wr[1].findAll('li')]\n",
    "        except:\n",
    "            writers = ''\n",
    "        try:\n",
    "            director = [w.text.split('(')[0] + ' ' + w.a['href'].split('nm')[1].split('/?')[0] for w in wr[0].findAll('li')]\n",
    "        except:\n",
    "            director = ''\n",
    "    except:\n",
    "        writers = ''\n",
    "        director = ''\n",
    "    \n",
    "#     #try:\n",
    "#       #  description = soup.find('span', class_ = 'sc-16ede01-2 gXUyNh').text\n",
    "#    # except:\n",
    "#        # description = 'no description'\n",
    "    \n",
    "    try:\n",
    "        avg_review = soup.find('span', class_=\"sc-7ab21ed2-1 jGRxWM\").text\n",
    "    except:\n",
    "        avg_review = 20\n",
    "    \n",
    "    try:\n",
    "        raw = soup.find('div', class_=\"sc-7ab21ed2-3 dPVcnq\").text\n",
    "        if raw[-1] == 'M':\n",
    "            num_reviews = float(raw[:-1]) * 1000000\n",
    "        elif raw[-1] == 'K':\n",
    "            num_reviews = float(raw[:-1]) * 1000\n",
    "        else:\n",
    "            num_reviews = float(raw)\n",
    "    except:\n",
    "        num_reviews = 0\n",
    "#     #try:\n",
    "#         #poster = soup.find('div', class_ = 'ipc-media ipc-media--poster-27x40 ipc-i\n",
    "#         #mage-media-ratio--poster-27x40 ipc-media--baseAlt ipc-media--poster-l ipc-p\n",
    "#         #oster__poster-image ipc-media__img').img['src']\n",
    "#     #except:\n",
    "#         #poster = 'none'\n",
    "    \n",
    "    try:\n",
    "        budget = int(soup.find('button', text = 'Budget').findNext('div').text.partition('(')[0][1:].strip().replace(',', ''))\n",
    "    except:\n",
    "        try:\n",
    "            budget = int(soup.find('button', text = 'Budget').findNext('div').text[4:].split('(')[0].split()[0].replace(',', ''))\n",
    "        except:\n",
    "            budget = 0\n",
    "        \n",
    "#    # try:\n",
    "#       #  dom_gross = int(soup.find('button', text = 'Gross US & Canada').findNext('div').text.partition('(')[0][1:].strip().replace(',', ''))\n",
    "#    # except:\n",
    "#     #    dom_gross = 0\n",
    "        \n",
    "#     #try:\n",
    "#         #world_gross = int(soup.find('button', text = 'Gross worldwide').findNext('div').text.partition('(')[0][1:].strip().replace(',', ''))\n",
    "#     #except:\n",
    "#         #world_gross = 0\n",
    "        \n",
    "#     #try:\n",
    "#         #meta_score = int(soup.find('span', class_ = 'score-meta').text)\n",
    "#     #except:\n",
    "#         #meta_score = 0\n",
    "    \n",
    "    try:\n",
    "        rating = soup.find_all('a', class_ = 'ipc-link ipc-link--baseAlt ipc-link--inherit-color sc-8c396aa2-1 WIUyh')[1].text\n",
    "    except:\n",
    "        rating = 'UNK'\n",
    "        \n",
    "    try:\n",
    "        date = soup.find('a', text = 'Release date').findNext('div').text.partition('(')[0].strip()\n",
    "    except:\n",
    "        date = '?'\n",
    "        \n",
    "    try:\n",
    "        country = [soup.find('button', text = 'Country of origin').findNext('div').text]\n",
    "    except:\n",
    "        try:\n",
    "            countries_raw = soup.find('button', text = 'Countries of origin').next_sibling()[0].findAll('li')\n",
    "            country = [c.text for c in countries_raw]\n",
    "        except:\n",
    "            country = ['unknown']\n",
    "    \n",
    "    genres = []\n",
    "    try:\n",
    "        gs = soup.find_all('a', class_ ='sc-16ede01-3 bYNgQ ipc-chip ipc-chip--on-baseAlt')\n",
    "        for g in gs:\n",
    "            genres.append(g.text)\n",
    "    except:\n",
    "        genres = ['none']\n",
    "       \n",
    "    all_comps = []\n",
    "    try:\n",
    "        comps = soup.find('a', text = 'Production companies').findNext('div').find_all('li')\n",
    "        for comp in comps:\n",
    "            all_comps.append(comp.text)\n",
    "    except:\n",
    "        all_comps = ['none']\n",
    "    \n",
    "    all_cast = []\n",
    "    try:\n",
    "        try:\n",
    "            cast = soup.findAll('a', class_ = 'sc-bfec09a1-1 gfeYgX')[:10]\n",
    "        except:\n",
    "            cast = soup.findAll('a', class_ = 'sc-bfec09a1-1 gfeYgX')\n",
    "            \n",
    "        for act in cast:\n",
    "            try:\n",
    "                all_cast.append(act.text + ' ' + act['href'].split('nm')[1].split('/?')[0])\n",
    "            except:\n",
    "                all_cast.append('no info')\n",
    "    except:\n",
    "        all_cast = ['none']\n",
    "    \n",
    "    try:\n",
    "        runtime = soup.find('button', text = 'Runtime').findNext('div').text.split()\n",
    "        run = int(runtime[0]) * 60\n",
    "        if len(runtime) > 2:\n",
    "            run = run + int(runtime[2])\n",
    "    except:\n",
    "        run = 0\n",
    "    \n",
    "    all_langs = []\n",
    "    try:\n",
    "        langs = soup.find('button', text = 'Languages').findNext('div').ul.find_all('li')\n",
    "        for lang in langs:\n",
    "            all_langs.append(lang.text)\n",
    "    except:\n",
    "        try:\n",
    "            all_langs.append(soup.find('button', text = 'Language').findNext('div').ul.li.text)\n",
    "        except:\n",
    "            all_langs = ['none']\n",
    "    \n",
    "    # this will be put into a dataframe later\n",
    "    basic_rows.append({'id' : movie_url[9:-1], 'title': title, 'writer': writers, 'director': director, 'actors': all_cast,\n",
    "                       'num_review': num_reviews,'release_date': date, 'country': country,# 'description': description, 'm_rating': rating,\n",
    "                          'imdb_rating': avg_review, #'meta_score': meta_score, #'poster': poster, \n",
    "                          'languages': all_langs, 'runtime': run, 'genres': genres, 'budget': budget,\n",
    "                          # 'dom_gross': dom_gross, 'world_gross': world_gross,\n",
    "                          'prod_comp': all_comps})\n",
    "    print(len(basic_rows), title, end = '\\r')\n",
    "    completed.append(movie_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddf452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the keywords for an individual movie\n",
    "def get_keys(movie_url):\n",
    "    if movie_url in completed_keys:\n",
    "        return 0\n",
    "\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent, 'Accept-Language': 'en-US,en;q=0.5'} \n",
    "  \n",
    "    req = requests.get('https://www.imdb.com/' + movie_url + 'keywords', headers = headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "    if str(req) != '<Response [200]>':\n",
    "        return 0\n",
    "\n",
    "    all_words = []\n",
    "    try:\n",
    "        words = soup.find_all('div', class_ = 'sodatext')\n",
    "        \n",
    "        for word in words:\n",
    "            all_words.append(word.text.strip())\n",
    "    except:\n",
    "           pass\n",
    "    \n",
    "    key_word_rows.append({'id': movie_url[9:-1], 'key_words': all_words})\n",
    "    print(len(key_word_rows))\n",
    "    completed_keys.append(movie_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell gets all of the movies from every genre\n",
    "get_all_genre_urls('comedy', 2080, all_u)\n",
    "with open('comedy.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('sci-fi', 325, all_u)\n",
    "with open('sci-fi.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('horror', 720, all_u)\n",
    "with open('horror.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('romance', 1030, all_u)\n",
    "with open('romance.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('action', 1035, all_u)\n",
    "with open('action.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('thriller', 1050, all_u)\n",
    "with open('thriller.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('drama', 4440, all_u)\n",
    "with open('drama.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('mystery', 366, all_u)\n",
    "with open('mystery.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('crime', 700, all_u)\n",
    "with open('crime.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "\n",
    "get_all_genre_urls('animation', 160, all_u)\n",
    "with open('animation.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('adventure', 500, all_u)\n",
    "with open('adventure.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "\n",
    "get_all_genre_urls('fantasy', 335, all_u)\n",
    "with open('fantasy.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "\n",
    "#get_all_genre_urls('biography', 160, all_u)\n",
    "get_all_genre_urls('family', 335, all_u)\n",
    "with open('family.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('history', 174, all_u)\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "\n",
    "get_all_genre_urls('war', 193, all_u)\n",
    "with open('war.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)\n",
    "    \n",
    "get_all_genre_urls('western', 179, all_u)\n",
    "with open('western.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc93675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the master list of movie urls\n",
    "with open('master_url_list.pkl', 'wb') as f:\n",
    "    pickle.dump(all_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76127740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421639"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acfbffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the data from all of the movie urls in all_u\n",
    "def get_movie_data(urls, start, finish):\n",
    "    basic_rows.clear()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        futures = [executor.submit(get_movie, u) for u in all_u[start:finish]]\n",
    "        wait(futures)\n",
    "        print(len(basic_rows))\n",
    "        print('thread done')\n",
    "    \n",
    "    print(len(basic_rows))\n",
    "\n",
    "    print('done2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed177edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_data(urls, start, finish):\n",
    "    key_word_rows.clear()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        futures = [executor.submit(get_keys, u) for u in all_u[start:finish]]\n",
    "        wait(futures)\n",
    "        print('thread done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c30e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21624 Apocalypse CowboyGolden Westeersrsrairie (Das Vermachtnis der Prarie)ear Bakersfieldnskikh sovietskikh zemel\n",
      "thread done\n",
      "21624\n",
      "done2\n"
     ]
    }
   ],
   "source": [
    "get_movie_data(all_u, 0, 421630)\n",
    "get_key_data(all_u, 0, 421630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3350f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the rows into a df\n",
    "df = pd.DataFrame(columns = ['id', 'title', 'release_date', 'runtime', 'm_rating', 'imdb_rating', 'num_review', 'director', \n",
    "                             'writer', 'actors', 'genres' , 'country', 'languages', 'prod_comp', 'budget'])\n",
    "for row in master_basic_rows:\n",
    "    print(len(df), end = '\\r')\n",
    "    df = df.append(row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key = pd.DataFrame(columns = ['id', 'key_words'])\n",
    "for row in key_word_rows:\n",
    "    print(len(df_key), end = '\\r')\n",
    "    df_key = df_key.append(row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cc0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell merges the keys and basic rows to get the final df\n",
    "films_df = pd.merge(df, all_keys, how = 'outer', on = 'id')\n",
    "films_df = films_df.dropna(subset = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a298b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is all bout getting rid of the movies that don't have enough data for me to work with\n",
    "\n",
    "films = deepcopy(films_df)\n",
    "films = films.drop_duplicates(subset= ['id'])\n",
    "films = films[films.release_date != '?']\n",
    "films = films[films.runtime != 0]\n",
    "films.release_date = pd.to_datetime(films.release_date)\n",
    "films.release_date = films.release_date.map(lambda x: x if x.year < 2023 else 2099)\n",
    "films = films[films.release_date != 2099]\n",
    "#films.release_date = films.release_date.apply(lambda x: x.toordinal())\n",
    "films = films[films.imdb_rating != 20]\n",
    "#films.locations = films.locations.map(lambda x: [l.split(',')[-1] for l in x])\n",
    "films = films[films.num_review > 100]\n",
    "films[['runtime', 'num_review', 'budget']] = films[['runtime', 'num_review', 'budget']].astype(float)\n",
    "films.imdb_rating = films.imdb_rating.astype(float)\n",
    "#films = films[films.budget != 0]\n",
    "films.dropna(inplace = True)\n",
    "films.key_words = films.key_words.map(lambda x: [w.lower() for w in x])\n",
    "films = films.reset_index().drop('index', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6086069",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_genres_df.pkl', 'wb') as f: \n",
    "    pickle.dump(films.drop('budget', axis = 1), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e09370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
